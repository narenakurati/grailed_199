---
title: "grailed_199_rmd"
author: "Naren Akurati"
date: "1/15/2019"
output: html_document
---

```{r, echo = FALSE}
library(testthat)
suppressPackageStartupMessages(library(ggplot2))
suppressPackageStartupMessages(library(dplyr))
suppressPackageStartupMessages(library(gridExtra))
library(class)
library(ISLR)
suppressPackageStartupMessages(library(caret))
library(e1071)
suppressPackageStartupMessages(library(MASS))
library(reshape2)
library(ggcorrplot)
suppressPackageStartupMessages(library(boot))
suppressPackageStartupMessages(library(resample))
library(rpart)
library(tree)
suppressPackageStartupMessages(library(randomForest))
library(rvest)
library(stringr)
```

#Using rvest to pull first load page of data
```{r, warning=FALSE, tidy.opts=list(width.cutoff=80), tidy = TRUE}
grailed <- read_html("https://www.grailed.com/sold/")
grailed

sample_url <- read_html('https://www.imdb.com/search/title?count=100&release_date=2016,2016&title_type=feature')

#test <- grailed %>% html_nodes("homepage-wrapper") %>% html_nodes("feed-item")
#test <- grailed %>% html_nodes('img')
#test <- grailed %>% html_nodes('.feed-item:nth-child(2)')
test <- grailed %>% html_nodes('.feed-item:nth-child(1) img') %>% html_attr("href")
test <- grailed %>% html_nodes('feed-item') %>% html_attr("href")
test <- grailed %>% html_attr("img")

#.feed-item~ .feed-item+ .feed-item img , .feed-item:nth-child(1) img

test <- read_html("https://www.grailed.com/sold/")
html_nodes(test, "feed-item")
html_attr(test, "class")
```

#After navigating to item page
```{r, warning=FALSE, tidy.opts=list(width.cutoff=80), tidy = TRUE}
#specific item

sold_item_test <- read_html('https://www.grailed.com/listings/7804580-Acne-Studios-Acne-Studios-Kai-Reverse-Pav16-Pre-Beige-Melange-wool-sweater')
sold_item_test <- read_html('https://www.grailed.com/listings/8129890-Nike---Off-White-Nike-x-Off-White-97-menta')

title <- sold_item_test %>% html_nodes('title') %>% html_text()
price <- sold_item_test %>% html_nodes('._sold') %>% html_text()
brand <- sold_item_test %>% html_nodes('.jumbo a') %>% html_text()
size <- sold_item_test %>% html_nodes('.listing-size') %>% html_text()
#NW user <- sold_item_test %>% html_nodes('.ListingSellerCard') %>% html_text()

script <- sold_item_test %>% html_nodes('script') %>% html_text()

# script[6]
# 
# "sold_at" %in% toString(script[6])
# toString(script[6])
# grelp("sold_at", toString(script[6]))
# 
# for (i in 1:length(script)){
#   'sold_at' %in% script[i]
# }

#we can pull time from this with "sold_at"
#we can also pull user from "username"

time <- script[7]
time <- as.character(time)
time <- regmatches(time, gregexpr("(?<=sold_at).*(?=sold_price.)", time, perl = TRUE))
time <- gsub("[\"]","", time); time <- gsub(",","", time); time <- str_sub(time, 2)

user <- script[7]
user <- as.character(user)
user <- regmatches(user, gregexpr("(?<=username).*(?=avatar_url)", user, perl = TRUE))
user <- gsub("[\"]","", user); user <- gsub(",","", user); user <- str_sub(user, 2)

data <- setNames(data.frame(matrix(ncol = 6, nrow = 1000)), c("title", "price", "brand", "size", "size", "time"))
#data$title[1] <- title
#data
#write.csv(time, file = "beans.txt")
```

#A function that will take URL as input as pull all relavent data
```{r, warning=FALSE, tidy.opts=list(width.cutoff=80), tidy = TRUE}
data <- setNames(data.frame(matrix(ncol = 6, nrow = 10)), c("title", "price", "brand", "size", "size", "time"))

pull_and_store <- function(url){
  
  temp <- matrix(ncol = 6, nrow = 1)
    
  sold_item_test <- read_html(url)
  
  title <- sold_item_test %>% html_nodes('title') %>% html_text()
  price <- sold_item_test %>% html_nodes('._sold') %>% html_text()
  brand <- sold_item_test %>% html_nodes('.jumbo a') %>% html_text()
  size <- sold_item_test %>% html_nodes('.listing-size') %>% html_text()
  
  script <- sold_item_test %>% html_nodes('script') %>% html_text()
  
  time <- script[7]
  time <- as.character(time)
  time <- regmatches(time, gregexpr("(?<=sold_at).*(?=sold_price.)", time, perl = TRUE))
  time <- gsub("[\"]","", time); time <- gsub(",","", time); time <- str_sub(time, 2)
  
  user <- script[7]
  user <- as.character(user)
  user <- regmatches(user, gregexpr("(?<=username).*(?=avatar_url)", user, perl = TRUE))
  user <- gsub("[\"]","", user); user <- gsub(",","", user); user <- str_sub(user, 2)
  
  temp[1,1] <- title
  temp[1,2] <- price
  temp[1,3] <- brand
  temp[1,4] <- size
  temp[1,5] <- time
  temp[1,6] <- user
  
  return(temp)
}

url <- 'https://www.grailed.com/listings/7804580-Acne-Studios-Acne-Studios-Kai-Reverse-Pav16-Pre-Beige-Melange-wool-sweater'
pull_and_store(url)
data[1,] <- pull_and_store(url)
```

#Testing function
```{r}
#clear dataframe
data <- setNames(data.frame(matrix(ncol = 6, nrow = 160)), c("title", "price", "brand", "size", "size", "time"))

#ready urls
url1 <- 'https://www.grailed.com/listings/7804580-Acne-Studios-Acne-Studios-Kai-Reverse-Pav16-Pre-Beige-Melange-wool-sweater'

url2 <- 'https://www.grailed.com/listings/6914524-John-Elliott--Kempy--Black-Wool-With-Leather-Overcoat-Coat-Size-1-S'

url3 <- 'https://www.grailed.com/listings/7375573-Undercover-1999-Flannel-Trousers'

url4 <- 'https://www.grailed.com/listings/7403289-John-Elliott-Camel-Wool-Topcoat'

url5 <- 'https://www.grailed.com/listings/7245404-Sandro-Calf-Leather-Jacket--Cognac--SS15'

data[1,] <- pull_and_store(url1)
data[2,] <- pull_and_store(url2)
data[3,] <- pull_and_store(url3)
data[4,] <- pull_and_store(url4)
data[5,] <- pull_and_store(url5)

data
```

#Automate data retrieval w PhantomJS
```{r, warning=FALSE, tidy.opts=list(width.cutoff=80), tidy = TRUE}
# sold_url <- 'https://www.grailed.com/sold'
# sold_page <- read_html(sold_url)
# image_url <- sold_page %>% html_nodes('body class') %>% html_text()# %*% html_attr('href')
# image_url
# image_url <- sold_page %>% html_nodes(xpath = '//*[@id="homepage-v2"]/div/div/div[2]/div/div[2]/div/div[2]/div[1]')# %>% html_attr('href')

url <- paste0('https://www.grailed.com/sold')
lines <- readLines("scrape_sold.js")
lines[1] <- paste0("var url ='", url , "';")
writeLines(lines, "scrape_sold.js")

system("./phantomjs scrape_sold.js")

sold_page <- read_html("1.html")

first_url <- sold_page %>% html_nodes(xpath = '//*[@id="homepage-v2"]/div/div/div[2]/div/div[2]/div/div[2]/div[1]/a') %>% xml_attr("href")
first_url

second_url <- sold_page %>% html_nodes(xpath = '//*[@id="homepage-v2"]/div/div/div[2]/div/div[2]/div/div[2]/div[2]/a') %>% xml_attr("href")

third_url <- sold_page %>% html_nodes(xpath = '//*[@id="homepage-v2"]/div/div/div[2]/div/div[2]/div/div[2]/div[3]/a') %>% xml_attr("href")

fourth_url <- sold_page %>% html_nodes(xpath = '//*[@id="homepage-v2"]/div/div/div[2]/div/div[2]/div/div[2]/div[4]/a') %>% xml_attr("href")

fifth_url <- sold_page %>% html_nodes(xpath = '//*[@id="homepage-v2"]/div/div/div[2]/div/div[2]/div/div[2]/div[5]/a') %>% xml_attr("href")

test <- sold_page %>% html_nodes(xpath = '//*[@id="homepage-v2"]/div/div/div[2]/div/div[2]/div/div[2]/div[80]/a') %>% xml_attr("href")
```

```{r, warning=FALSE, tidy.opts=list(width.cutoff=80), tidy = TRUE}
#function to get all 80 guys
load_data <- function(){
  
  #create temp frame
  temp_frame <- setNames(data.frame(matrix(ncol = 6, nrow = 80)), c("title", "price", "brand", "size", "time", "user"))
  
  #redefining function
  pull_and_store_internal <- function(url){
  
    temp <- matrix(ncol = 6, nrow = 1)
      
    sold_item_test <- read_html(url)
    
    title <- sold_item_test %>% html_nodes('title') %>% html_text()
    price <- sold_item_test %>% html_nodes('._sold') %>% html_text()
    brand <- sold_item_test %>% html_nodes('.jumbo') %>% html_text()
    size <- sold_item_test %>% html_nodes('.listing-size') %>% html_text()
    
    script <- sold_item_test %>% html_nodes('script') %>% html_text()
    
    time <- script[6]
    time <- as.character(time)
    time <- regmatches(time, gregexpr("(?<=sold_at).*(?=sold_price.)", time, perl = TRUE))
    time <- gsub("[\"]","", time); time <- gsub(",","", time); time <- str_sub(time, 2)
    
    user <- script[6]
    user <- as.character(user)
    user <- regmatches(user, gregexpr("(?<=username).*(?=avatar_url)", user, perl = TRUE))
    user <- gsub("[\"]","", user); user <- gsub(",","", user); user <- str_sub(user, 2)
    
    temp[1,1] <- title
    temp[1,2] <- price
    temp[1,3] <- brand
    temp[1,4] <- size
    temp[1,5] <- time
    temp[1,6] <- user
    
    return(temp)
  }
  
  #load up stuff
  url <- paste0('https://www.grailed.com/sold')
  lines <- readLines("scrape_sold.js")
  lines[1] <- paste0("var url ='", url , "';")
  writeLines(lines, "scrape_sold.js")
  system("./phantomjs scrape_sold.js")
  sold_page <- read_html("1.html")
  
  #iterate through 80 list
  for (i in 1:80){
    created_url <- paste("//*[@id=\"homepage-v2\"]/div/div/div[2]/div/div[2]/div/div[2]/div[", i , "]/a", sep = "")
    #print(created_url)
    temp_url <- sold_page %>% html_nodes(xpath = created_url) %>% xml_attr("href")
    temp_url <- paste("https://www.grailed.com", temp_url, sep = "")
    print(temp_url)
    temp_frame[i,] <- pull_and_store_internal(temp_url)
  }
  return(temp_frame)
}

first_run <- load_data()
second_run <- load_data()
third_run <- load_data()
fourth_run <- load_data()
fifth_run <- load_data()
sixth_run <- load_data()

rm(merged)
# merged <- join(first_run, second_run, by = "title", type = "left")
merged <- full_join(first_run, fifth_run, by = "title")
merged_1 <- full_join(first_run, sixth_run, by = "title")

ggplot(data = first_run, aes(x=factor(brand))) + geom_bar(stat="count")
```
