---
title: "grailed_199_rmd"
author: "Naren Akurati"
date: "1/15/2019"
output: html_document
---

```{r, echo = FALSE}
library(testthat)
suppressPackageStartupMessages(library(ggplot2))
suppressPackageStartupMessages(library(dplyr))
suppressPackageStartupMessages(library(gridExtra))
library(class)
library(ISLR)
suppressPackageStartupMessages(library(caret))
library(e1071)
suppressPackageStartupMessages(library(MASS))
library(reshape2)
library(ggcorrplot)
suppressPackageStartupMessages(library(boot))
suppressPackageStartupMessages(library(resample))
library(rpart)
library(tree)
suppressPackageStartupMessages(library(randomForest))
library(rvest)
library(stringr)
library(curl)
library(httr)
library(rjson)
library(parsedate)
```

#Using rvest to pull first load page of data
```{r, warning=FALSE, tidy.opts=list(width.cutoff=80), tidy = TRUE}
grailed <- read_html("https://www.grailed.com/sold/")
grailed

sample_url <- read_html('https://www.imdb.com/search/title?count=100&release_date=2016,2016&title_type=feature')

#test <- grailed %>% html_nodes("homepage-wrapper") %>% html_nodes("feed-item")
#test <- grailed %>% html_nodes('img')
#test <- grailed %>% html_nodes('.feed-item:nth-child(2)')
test <- grailed %>% html_nodes('.feed-item:nth-child(1) img') %>% html_attr("href")
test <- grailed %>% html_nodes('feed-item') %>% html_attr("href")
test <- grailed %>% html_attr("img")

#.feed-item~ .feed-item+ .feed-item img , .feed-item:nth-child(1) img

test <- read_html("https://www.grailed.com/sold/")
html_nodes(test, "feed-item")
html_attr(test, "class")
```

#After navigating to item page
```{r, warning=FALSE, tidy.opts=list(width.cutoff=80), tidy = TRUE}
#specific item

sold_item_test <- read_html('https://www.grailed.com/listings/7804580-Acne-Studios-Acne-Studios-Kai-Reverse-Pav16-Pre-Beige-Melange-wool-sweater')
sold_item_test <- read_html('https://www.grailed.com/listings/8129890-Nike---Off-White-Nike-x-Off-White-97-menta')

title <- sold_item_test %>% html_nodes('title') %>% html_text()
price <- sold_item_test %>% html_nodes('._sold') %>% html_text()
brand <- sold_item_test %>% html_nodes('.jumbo a') %>% html_text()
size <- sold_item_test %>% html_nodes('.listing-size') %>% html_text()
#NW user <- sold_item_test %>% html_nodes('.ListingSellerCard') %>% html_text()

script <- sold_item_test %>% html_nodes('script') %>% html_text()

# script[6]
# 
# "sold_at" %in% toString(script[6])
# toString(script[6])
# grelp("sold_at", toString(script[6]))
# 
# for (i in 1:length(script)){
#   'sold_at' %in% script[i]
# }

#we can pull time from this with "sold_at"
#we can also pull user from "username"

time <- script[7]
time <- as.character(time)
time <- regmatches(time, gregexpr("(?<=sold_at).*(?=sold_price.)", time, perl = TRUE))
time <- gsub("[\"]","", time); time <- gsub(",","", time); time <- str_sub(time, 2)

user <- script[7]
user <- as.character(user)
user <- regmatches(user, gregexpr("(?<=username).*(?=avatar_url)", user, perl = TRUE))
user <- gsub("[\"]","", user); user <- gsub(",","", user); user <- str_sub(user, 2)

data <- setNames(data.frame(matrix(ncol = 6, nrow = 1000)), c("title", "price", "brand", "size", "size", "time"))
#data$title[1] <- title
#data
#write.csv(time, file = "beans.txt")
```

#A function that will take URL as input as pull all relavent data
```{r, warning=FALSE, tidy.opts=list(width.cutoff=80), tidy = TRUE}
data <- setNames(data.frame(matrix(ncol = 6, nrow = 10)), c("title", "price", "brand", "size", "size", "time"))

pull_and_store <- function(url){
  
  temp <- matrix(ncol = 6, nrow = 1)
    
  sold_item_test <- read_html(url)
  
  title <- sold_item_test %>% html_nodes('title') %>% html_text()
  price <- sold_item_test %>% html_nodes('._sold') %>% html_text()
  brand <- sold_item_test %>% html_nodes('.jumbo a') %>% html_text()
  size <- sold_item_test %>% html_nodes('.listing-size') %>% html_text()
  
  script <- sold_item_test %>% html_nodes('script') %>% html_text()
  
  time <- script[7]
  time <- as.character(time)
  time <- regmatches(time, gregexpr("(?<=sold_at).*(?=sold_price.)", time, perl = TRUE))
  time <- gsub("[\"]","", time); time <- gsub(",","", time); time <- str_sub(time, 2)
  
  user <- script[7]
  user <- as.character(user)
  user <- regmatches(user, gregexpr("(?<=username).*(?=avatar_url)", user, perl = TRUE))
  user <- gsub("[\"]","", user); user <- gsub(",","", user); user <- str_sub(user, 2)
  
  temp[1,1] <- title
  temp[1,2] <- price
  temp[1,3] <- brand
  temp[1,4] <- size
  temp[1,5] <- time
  temp[1,6] <- user
  
  return(temp)
}

url <- 'https://www.grailed.com/listings/7804580-Acne-Studios-Acne-Studios-Kai-Reverse-Pav16-Pre-Beige-Melange-wool-sweater'
pull_and_store(url)
data[1,] <- pull_and_store(url)
```

#Testing function
```{r}
#clear dataframe
data <- setNames(data.frame(matrix(ncol = 6, nrow = 160)), c("title", "price", "brand", "size", "size", "time"))

#ready urls
url1 <- 'https://www.grailed.com/listings/7804580-Acne-Studios-Acne-Studios-Kai-Reverse-Pav16-Pre-Beige-Melange-wool-sweater'

url2 <- 'https://www.grailed.com/listings/6914524-John-Elliott--Kempy--Black-Wool-With-Leather-Overcoat-Coat-Size-1-S'

url3 <- 'https://www.grailed.com/listings/7375573-Undercover-1999-Flannel-Trousers'

url4 <- 'https://www.grailed.com/listings/7403289-John-Elliott-Camel-Wool-Topcoat'

url5 <- 'https://www.grailed.com/listings/7245404-Sandro-Calf-Leather-Jacket--Cognac--SS15'

data[1,] <- pull_and_store(url1)
data[2,] <- pull_and_store(url2)
data[3,] <- pull_and_store(url3)
data[4,] <- pull_and_store(url4)
data[5,] <- pull_and_store(url5)

data
```

#Automate data retrieval w PhantomJS
```{r, warning=FALSE, tidy.opts=list(width.cutoff=80), tidy = TRUE}
# sold_url <- 'https://www.grailed.com/sold'
# sold_page <- read_html(sold_url)
# image_url <- sold_page %>% html_nodes('body class') %>% html_text()# %*% html_attr('href')
# image_url
# image_url <- sold_page %>% html_nodes(xpath = '//*[@id="homepage-v2"]/div/div/div[2]/div/div[2]/div/div[2]/div[1]')# %>% html_attr('href')

url <- paste0('https://www.grailed.com/sold')
lines <- readLines("scrape_sold.js")
lines[1] <- paste0("var url ='", url , "';")
writeLines(lines, "scrape_sold.js")

system("./phantomjs scrape_sold.js")

sold_page <- read_html("1.html")

first_url <- sold_page %>% html_nodes(xpath = '//*[@id="homepage-v2"]/div/div/div[2]/div/div[2]/div/div[2]/div[1]/a') %>% xml_attr("href")
first_url

second_url <- sold_page %>% html_nodes(xpath = '//*[@id="homepage-v2"]/div/div/div[2]/div/div[2]/div/div[2]/div[2]/a') %>% xml_attr("href")

third_url <- sold_page %>% html_nodes(xpath = '//*[@id="homepage-v2"]/div/div/div[2]/div/div[2]/div/div[2]/div[3]/a') %>% xml_attr("href")

fourth_url <- sold_page %>% html_nodes(xpath = '//*[@id="homepage-v2"]/div/div/div[2]/div/div[2]/div/div[2]/div[4]/a') %>% xml_attr("href")

fifth_url <- sold_page %>% html_nodes(xpath = '//*[@id="homepage-v2"]/div/div/div[2]/div/div[2]/div/div[2]/div[5]/a') %>% xml_attr("href")

test <- sold_page %>% html_nodes(xpath = '//*[@id="homepage-v2"]/div/div/div[2]/div/div[2]/div/div[2]/div[80]/a') %>% xml_attr("href")
```

#Automate data retrieval function
```{r, warning=FALSE, tidy.opts=list(width.cutoff=80), tidy = TRUE}
#function to get all 80 guys
load_data <- function(){
  
  #create temp frame
  temp_frame <- setNames(data.frame(matrix(ncol = 6, nrow = 80)), c("title", "price", "brand", "size", "time", "user"))
  
  #redefining function
  pull_and_store_internal <- function(url){
  
    temp <- matrix(ncol = 6, nrow = 1)
      
    sold_item_test <- read_html(url)
    
    title <- sold_item_test %>% html_nodes('title') %>% html_text()
    price <- sold_item_test %>% html_nodes('._sold') %>% html_text()
    brand <- sold_item_test %>% html_nodes('.jumbo') %>% html_text()
    size <- sold_item_test %>% html_nodes('.listing-size') %>% html_text()
    
    script <- sold_item_test %>% html_nodes('script') %>% html_text()
    
    time <- script[6]
    time <- as.character(time)
    time <- regmatches(time, gregexpr("(?<=sold_at).*(?=sold_price.)", time, perl = TRUE))
    time <- gsub("[\"]","", time); time <- gsub(",","", time); time <- str_sub(time, 2)
    
    user <- script[6]
    user <- as.character(user)
    user <- regmatches(user, gregexpr("(?<=username).*(?=avatar_url)", user, perl = TRUE))
    user <- gsub("[\"]","", user); user <- gsub(",","", user); user <- str_sub(user, 2)
    
    temp[1,1] <- title
    temp[1,2] <- price
    temp[1,3] <- brand
    temp[1,4] <- size
    temp[1,5] <- time
    temp[1,6] <- user
    
    return(temp)
  }
  
  #load up stuff
  url <- paste0('https://www.grailed.com/sold')
  lines <- readLines("scrape_sold.js")
  lines[1] <- paste0("var url ='", url , "';")
  writeLines(lines, "scrape_sold.js")
  system("./phantomjs scrape_sold.js")
  sold_page <- read_html("1.html")
  
  #iterate through 80 list
  for (i in 1:80){
    created_url <- paste("//*[@id=\"homepage-v2\"]/div/div/div[2]/div/div[2]/div/div[2]/div[", i , "]/a", sep = "")
    #print(created_url)
    temp_url <- sold_page %>% html_nodes(xpath = created_url) %>% xml_attr("href")
    temp_url <- paste("https://www.grailed.com", temp_url, sep = "")
    print(temp_url)
    temp_frame[i,] <- pull_and_store_internal(temp_url)
  }
  return(temp_frame)
}

first_run <- load_data()
second_run <- load_data()
third_run <- load_data()
fourth_run <- load_data()
fifth_run <- load_data()
sixth_run <- load_data()

start_time <- Sys.time()
seventh_run <- load_data()
end_time <- Sys.time()

end_time - start_time

rm(merged)
# merged <- join(first_run, second_run, by = "title", type = "left")
merged <- full_join(first_run, fifth_run, by = "title")
merged_1 <- full_join(first_run, sixth_run, by = "title")

ggplot(data = first_run, aes(x=factor(brand))) + geom_bar(stat="count")
```

#CURL Package
```{r}
req <- curl_fetch_memory("https://mnrwefss2q-dsn.algolia.net/1/indexes/Listing_sold_production/query?x-algolia-agent=Algolia%20for%20vanilla%20JavaScript%203.22.3&x-algolia-application-id=MNRWEFSS2Q&x-algolia-api-key=a3a4de2e05d9e9b463911705fb6323ad' -H 'accept: application/json")

test <- curl_fetch_memory("https://www.grailed.com/sold/")

result <- POST("https://mnrwefss2q-dsn.algolia.net/1/indexes/Listing_sold_production/query",
               verbose(),
               encode="json",
               add_headers(`x-algolia-api-key`="a3a4de2e05d9e9b463911705fb6323ad"))

myURL <- sprintf('https://mnrwefss2q-dsn.algolia.net/1/indexes/Listing_sold_production/query')

rs <- httr::POST(
  url = myURL,
  add_headers(.headers = c("x-algolia-agent" = "Algolia for vanilla JavaScript 3.22.3")),
  add_headers(.headers = c("x-algolia-application-id" = "MNRWEFSS2Q")),
  add_headers(.headers = c("x-algolia-api-key" = "a3a4de2e05d9e9b463911705fb6323ad")))

rs$headers

POST("https://mnrwefss2q-dsn.algolia.net/1/indexes/Listing_sold_production/query?x-algolia-agent=Algolia%20for%20vanilla%20JavaScript%203.22.3&x-algolia-application-id=MNRWEFSS2Q&x-algolia-api-key=a3a4de2e05d9e9b463911705fb6323ad' -H 'accept: application/json")

req <- curl_fetch_memory("https://mnrwefss2q-dsn.algolia.net/1/indexes/Listing_sold_production/query")
str(req)

parse_headers(req$headers)

#jsonlite:prettify(rawToChar()))
```

##CURL (PARAMS WORKING)
```{r}
h <- new_handle()
#handle_setopt(h)

beans <- as.character('{"params":"query=&filters=(strata%3A\'grailed\'%20OR%20strata%3A\'hype\')%20AND%20(marketplace%3Agrailed)&hitsPerPage=20&facets=%5B%22strata%22%2C%22size%22%2C%22category%22%2C%22category_size%22%2C%22category_path%22%2C%22category_path_size%22%2C%22category_path_root_size%22%2C%22price_i%22%2C%22designers.id%22%2C%22location%22%2C%22marketplace%22%2C%22badges%22%5D&page=0"}')

handle_setform(h, params = beans)

handle_setheaders(h,
  "Content-Type" = "application/x-www-form-urlencoded"
)

req <- curl_fetch_memory("https://mnrwefss2q-dsn.algolia.net/1/indexes/Listing_sold_production/query?x-algolia-agent=Algolia%20for%20vanilla%20JavaScript%203.22.3&x-algolia-application-id=MNRWEFSS2Q&x-algolia-api-key=a3a4de2e05d9e9b463911705fb6323ad", handle = h)

req
```


##HTTR (POST REQUEST WORKING)
```{r}
# r <- POST("https://mnrwefss2q-dsn.algolia.net/1/indexes/Listing_sold_production/query?x-algolia-agent=Algolia%20for%20vanilla%20JavaScript%203.22.3&x-algolia-application-id=MNRWEFSS2Q&x-algolia-api-key=a3a4de2e05d9e9b463911705fb6323ad", body = beans, encode = "form", verbose())

r <- POST("https://mnrwefss2q-dsn.algolia.net/1/indexes/Listing_sold_production/query?x-algolia-agent=Algolia%20for%20vanilla%20JavaScript%203.22.3&x-algolia-application-id=MNRWEFSS2Q&x-algolia-api-key=a3a4de2e05d9e9b463911705fb6323ad", body = beans, encode = "form")

r

str(content(r))$hits

object <- content(r, "parsed")$hits

object[[1]]$title
object[[1]]$price
object[[1]]$sold_at
object[[1]]$user$username
object[[1]]$user$total_bought_and_sold
object[[1]]$designer$name
object[[1]]$sold_price

collected_data <- data.frame("title" = c(1:500), "price" = c(1:500), "sold_at" = c(1:500), "username" = c(1:500), "bought_and_sold" = c(1:500), "designer" = c(1:500), "sold_price" = c(1:500))
collected_data

new_data <- data.frame("title", "price", "sold_at", "username", "bought_and_sold", "designer", "sold_price")

object[[1]]$title

for (i in 1:20){
  collected_data$price[i] <- object[[i]]$title
  # object[[i]]$price
  # object[[i]]$sold_at
  # object[[i]]$user$username
  # object[[i]]$user$total_bought_and_sold
  # object[[i]]$designer$name
  # object[[i]]$sold_price
}

collected_data

result <- toJSON(object)
test <- fromJSON(result)

#write(result, "test.json")

str(content(r)$hits) #<- gucci
json <- str(content(r)$hits)
json

#test <- readLines(str(content(r)$hits))

head(r)

content(r)$hits[[1]]

ggplot(data = first_run, aes(x=factor(brand))) + geom_bar(stat="count")
```

##CLEAN
```{r}
#get 999 hits
beans <- as.character('{"params":"query=&filters=(strata%3A\'grailed\'%20OR%20strata%3A\'hype\')%20AND%20(marketplace%3Agrailed)&hitsPerPage=999&facets=%5B%22strata%22%2C%22size%22%2C%22category%22%2C%22category_size%22%2C%22category_path%22%2C%22category_path_size%22%2C%22category_path_root_size%22%2C%22price_i%22%2C%22designers.id%22%2C%22location%22%2C%22marketplace%22%2C%22badges%22%5D&page=0"}')

#pull request
r <- POST("https://mnrwefss2q-dsn.algolia.net/1/indexes/Listing_sold_production/query?x-algolia-agent=Algolia%20for%20vanilla%20JavaScript%203.22.3&x-algolia-application-id=MNRWEFSS2Q&x-algolia-api-key=a3a4de2e05d9e9b463911705fb6323ad", body = beans, encode = "form")

#save(r, file = "two.RData")
#write.csv(collected_data, file = "current.csv")

#parse the hits
object <- content(r, "parsed")$hits

#set up dataframe
collected_data <- data.frame("title" = c(1:999), "price" = c(1:999), "sold_at" = c(1:999), "username" = c(1:999), "bought_and_sold" = c(1:999), "designer" = c(1:999), "sold_price" = c(1:999))

#fill dataframe
for (i in 1:999){
  collected_data$title[i] <- object[[i]]$title
  collected_data$price[i] <- object[[i]]$price
  collected_data$sold_at[i] <- object[[i]]$sold_at
  collected_data$username[i] <- object[[i]]$user$username
  collected_data$bought_and_sold[i] <- object[[i]]$user$total_bought_and_sold
  collected_data$designer[i] <- object[[i]]$designer$name
  collected_data$sold_price[i] <- object[[i]]$sold_price
}

collected_data


#remove outlier point (not always needed)
#which(collected_data$sold_at == min(as.Date(collected_data$sold_at)))
#collected_data <- collected_data[-which(collected_data$sold_at == min(as.Date(collected_data$sold_at))),]

#time range (about 16 hours)
range(collected_data$sold_at)

#determine top 10 brands
collected_data %>% count(designer, sort = TRUE, n = 10)
ggplot(data = collected_data, aes(x=factor(designer))) + geom_bar(stat="count")
ggplot(data = collected_data, aes(x=as.Date(sold_at), y=factor(designer))) + geom_point()

#merge
#merged <- full_join(first_run, fifth_run, by = "title")

#create time intervals and get count of brands in that time
range(collected_data$sold_at)
max(collected_data$sold_at)
parse_date(max(collected_data$sold_at))

#hour interval
x <- 3

  #hist(parse_date(collected_data$sold_at), "hours")
parsed_dates <- parse_date(collected_data$sold_at)
collected_data_reorder <- collected_data[order(collected_data$sold_at),]
collected_data_reorder$sold_at <- parse_date(collected_data_reorder$sold_at)

#create three hour interval
collected_data_reorder$sold_at[1]

#assign interval values
max <- as.numeric(trunc((max(collected_data_reorder$sold_at) - min(collected_data_reorder$sold_at)) / (x/24)))

min(collected_data_reorder$sold_at) + x*60*60

which[collected_data_reorder$sold_at]

i <- 1
j <- 1
for (i <= max){
  for (j in length(collected_data_reorder)){
    if(collected_data_reorder$sold_at[j] <= i*x*60*60){
      collected_data_reorder$interval[]
      collected_data_reorder$sold_at[1] + x*60*60
    }
  }
}



```
